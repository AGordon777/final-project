{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e1a003",
   "metadata": {},
   "source": [
    "# Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2247bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3938cc",
   "metadata": {},
   "source": [
    "We will build a Regression model to predict GDP/capita based on the 'balance' dataset prepared in 'data-cleaning-processing.ipynb'. This includes the more recent data so will be valid for the yeats 2010-2020 inclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c558ac3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'raw data/national-gdp-constant-usd-wb.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# source data https://ourworldindata.org/grapher/national-gdp-constant-usd-wb\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m gdp \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw data/national-gdp-constant-usd-wb.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m gdp\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'raw data/national-gdp-constant-usd-wb.csv'"
     ]
    }
   ],
   "source": [
    "# source data https://ourworldindata.org/grapher/national-gdp-constant-usd-wb\n",
    "\n",
    "gdp = pd.read_csv('raw data/national-gdp-constant-usd-wb.csv')\n",
    "gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp.columns = ['area','iso_alpha3_code','year','gdp']\n",
    "\n",
    "gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4014b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gdp.shape)\n",
    "display(gdp.dtypes)\n",
    "gdp.isna().sum()      # there are NaN values in the 'iso_alpha3_code' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we can see that these are largely for aggregated groups not present in the 'balance' dataset\n",
    "\n",
    "gdp[gdp.iso_alpha3_code.isna()==True].area.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8699486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see good data coverage for countries in years that overlap with the 'balance' dataset\n",
    "\n",
    "gdp[gdp.iso_alpha3_code.isna()==False].year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance = pd.read_csv('presentation data/national_balance.csv', encoding='ISO-8859-1').drop(['Unnamed: 0'], axis = 1)\n",
    "display(balance.shape)\n",
    "balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5886f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(balance.isna().sum())\n",
    "balance.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance.columns = [x.lower() for x in balance.columns]\n",
    "balance.columns = balance.columns.str.replace(\" \", \"_\").str.replace(\"-\", \"_\")\n",
    "balance.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044084cc",
   "metadata": {},
   "source": [
    "I want to include some calculated fields in the model data. This may increase colinearity between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance['kcal_per_capita_per_day'] = balance['food_supply_kcal_per_day_grand_total'] / balance['population']\n",
    "balance['protein_g_per_capita_per_day'] = balance['protein_supply_g_per_day_grand_total'] / balance['population']\n",
    "balance['fat_g_per_capita_per_day'] = balance['fat_supply_g_per_day_grand_total'] / balance['population']\n",
    "balance['animal_proportion'] = balance['food_supply_kcal_per_day_animal_products'] / balance['food_supply_kcal_per_day_grand_total']\n",
    "\n",
    "balance = balance.drop(['area',\n",
    "                        'fat_supply_g_per_day_animal_products',\n",
    "                        'fat_supply_g_per_day_grand_total',\n",
    "                        'fat_supply_g_per_day_vegetal_products',\n",
    "                        'food_supply_kcal_per_day_animal_products',\n",
    "                        'food_supply_kcal_per_day_grand_total',\n",
    "                        'food_supply_kcal_per_day_vegetal_products',\n",
    "                        'protein_supply_g_per_day_animal_products',\n",
    "                        'protein_supply_g_per_day_grand_total',\n",
    "                        'protein_supply_g_per_day_vegetal_products'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(balance.shape)\n",
    "display(balance.head())\n",
    "balance.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81d2b1",
   "metadata": {},
   "source": [
    "We have generated some NaN values within these calculated fields that need to be treated. It appears that these are all for rows where 'population' is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d256fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance[balance.kcal_per_capita_per_day.isna()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ead1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity, we will just drop these from the model\n",
    "balance.drop(balance[balance.kcal_per_capita_per_day.isna()==True].index, inplace = True)\n",
    "balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8834e6",
   "metadata": {},
   "source": [
    "I will also include some columns from another dataset prepared earlier. Again, this contains calculated fields which may increase the colinearity of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = pd.read_csv('presentation data/trade.csv', encoding='ISO-8859-1').drop(['Unnamed: 0'], axis = 1)\n",
    "display(trade.shape)\n",
    "display(trade.head())\n",
    "trade.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4964247",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = trade.groupby(['iso_alpha3_code','year']).agg({'Total Population - Both sexes_1000 persons':np.mean,'kcal_produced':sum, 'kcal_imported':sum, 'kcal_exported':sum, 'kcal_lost':sum}).reset_index()\n",
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade['kcal_prod_per_capita_per_day'] = trade['kcal_produced'] / (365 * 1000 * trade['Total Population - Both sexes_1000 persons'])\n",
    "trade['kcal_imp_per_capita_per_day'] = trade['kcal_imported'] / (365 * 1000 * trade['Total Population - Both sexes_1000 persons'])\n",
    "trade['kcal_exp_per_capita_per_day'] = trade['kcal_exported'] / (365 * 1000 * trade['Total Population - Both sexes_1000 persons'])\n",
    "trade['kcal_lost_per_capita_per_day'] = trade['kcal_lost'] / (365 * 1000 * trade['Total Population - Both sexes_1000 persons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9437a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = trade.drop(['Total Population - Both sexes_1000 persons',\n",
    "                   'kcal_produced',\n",
    "                   'kcal_imported',\n",
    "                   'kcal_exported',\n",
    "                   'kcal_lost'], axis=1)\n",
    "display(trade.shape)\n",
    "trade.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbcdb1f",
   "metadata": {},
   "source": [
    "Again, this generated nulls in the calculated fields where 'population' was 0. As we had already dropped them from one part of the dataset, it made sense to do the same here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d96b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade.drop(trade[trade.kcal_prod_per_capita_per_day.isna()==True].index, inplace = True)\n",
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdab324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining these features\n",
    "\n",
    "data = pd.merge(left = balance, right = trade, how = 'left', on = ['iso_alpha3_code', 'year'])\n",
    "display(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e3a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe36f7",
   "metadata": {},
   "source": [
    "Combining the food availability data with the target GDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(left = data, right = gdp, how = 'left', on = ['iso_alpha3_code','year']).drop(['area'],axis=1)\n",
    "display(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bcf3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gdp_per_capita'] = data['gdp'] / data['population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35211c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data.gdp.isna()==True].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b848623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('presentation data/model_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8db685",
   "metadata": {},
   "source": [
    "We now have a dataset on which to build a Regression Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1d61c",
   "metadata": {},
   "source": [
    "### Scaling and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10921285",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['iso_alpha3_code','gdp','gdp_per_capita'], axis=1).copy()\n",
    "\n",
    "y = data['gdp_per_capita']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89495bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering the 'sub_region_name' column\n",
    "regions = pd.DataFrame(X.sub_region_name.value_counts(normalize=True))\n",
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll group the lower frequency regions (less than 5% data each) into \"Other\"\n",
    "less_freq_regions = regions[regions['sub_region_name']<0.05].index.tolist()\n",
    "\n",
    "X[\"sub_region_name\"].loc[X[\"sub_region_name\"].isin(less_freq_regions)] = \"Other\"\n",
    "\n",
    "X.sub_region_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f697154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is categorical, but can be included in numericals for scaling\n",
    "# it is also imbalanced, so we might improve by oversampling\n",
    "\n",
    "X.least_developed_countries_ldc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c9dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.least_developed_countries_ldc = X.least_developed_countries_ldc.replace('x',1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this column will be treated as a categorical and OneHot encoded\n",
    "X.year = X.year.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "display(X_train.head())\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4222bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train.select_dtypes(np.number).copy()\n",
    "X_test_num = X_test.select_dtypes(np.number).copy()\n",
    "X_train_cat = X_train.select_dtypes(object).copy()\n",
    "X_test_cat = X_test.select_dtypes(object).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will scale the numerical data with MinMax\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "transformer = MinMaxScaler().fit(X_train_num)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(transformer.transform(X_train_num), columns=X_train_num.columns)\n",
    "X_test_scaled = pd.DataFrame(transformer.transform(X_test_num), columns=X_train_num.columns)\n",
    "\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a355aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d841551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will encode the categoricals (excluding 'least_developed_countries_ldc') using OneHot\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(drop='first').fit(X_train_cat)\n",
    "cols = encoder.get_feature_names_out(input_features=X_train_cat.columns)\n",
    "\n",
    "X_train_encode = pd.DataFrame(encoder.transform(X_train_cat).toarray(),columns=cols)\n",
    "X_test_encode = pd.DataFrame(encoder.transform(X_test_cat).toarray(),columns=cols)\n",
    "\n",
    "X_train_encode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd14fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = pd.concat([X_train_scaled,X_train_encode], axis = 1)\n",
    "X_test_transformed = pd.concat([X_test_scaled,X_test_encode], axis = 1)\n",
    "\n",
    "y_train = y_train.reset_index(drop=True)          \n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "display(y_train.head())\n",
    "X_train_transformed.head()      # this is our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed.head()      # this is our test/validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ec9c6",
   "metadata": {},
   "source": [
    "### Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "LR = LinearRegression().fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = LR.predict(X_test_transformed)\n",
    "\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d051676f",
   "metadata": {},
   "source": [
    "A moderate score, but we can certainly improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to prepare data as above:\n",
    "\n",
    "def yX_transform(X_train, X_test, y_train, y_test):  \n",
    "    X_train_num = X_train.select_dtypes(np.number).copy()\n",
    "    X_test_num = X_test.select_dtypes(np.number).copy()\n",
    "    X_train_cat = X_train.select_dtypes(object).copy()\n",
    "    X_test_cat = X_test.select_dtypes(object).copy()\n",
    "    \n",
    "    transformer = MinMaxScaler().fit(X_train_num)\n",
    "    X_train_scaled = pd.DataFrame(transformer.transform(X_train_num), columns=X_train_num.columns)\n",
    "    X_test_scaled = pd.DataFrame(transformer.transform(X_test_num), columns=X_train_num.columns)\n",
    "    \n",
    "    encoder = OneHotEncoder(drop='first').fit(X_train_cat)\n",
    "    cols = encoder.get_feature_names_out(input_features=X_train_cat.columns)\n",
    "    X_train_encode = pd.DataFrame(encoder.transform(X_train_cat).toarray(),columns=cols)\n",
    "    X_test_encode = pd.DataFrame(encoder.transform(X_test_cat).toarray(),columns=cols)\n",
    "    \n",
    "    X_train_transformed = pd.concat([X_train_scaled,X_train_encode], axis = 1)\n",
    "    X_test_transformed = pd.concat([X_test_scaled,X_test_encode], axis = 1)\n",
    "\n",
    "    y_train = y_train.reset_index(drop=True)          # to realign the X and y datasets\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train_transformed, X_test_transformed, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9183c",
   "metadata": {},
   "source": [
    "### Improving the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4613e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec124f",
   "metadata": {},
   "source": [
    "Clearly the population and the kcal_X_per_capita_per_day data are heavily skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['population','kcal_prod_per_capita_per_day','kcal_exp_per_capita_per_day','kcal_imp_per_capita_per_day','kcal_lost_per_capita_per_day']\n",
    "for feat in features:\n",
    "    sns.boxplot(data = X_train_transformed, x=feat)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e048e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are clear outliers in population, kcal_prod and kcal_lost\n",
    "\n",
    "X_train[['kcal_prod_per_capita_per_day','kcal_exp_per_capita_per_day','kcal_imp_per_capita_per_day','kcal_lost_per_capita_per_day']].sort_values(by=['kcal_lost_per_capita_per_day'], ascending=False).head(20)\n",
    "\n",
    "# row 86 appears erroneous so will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['population']].sort_values(by=['population'], ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec655bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as China and India are so much more populous than any other country, they are likely skewing the data\n",
    "\n",
    "# let's try removing these rows\n",
    "\n",
    "drop_index = X_train[['population']].sort_values(by=['population'], ascending=False).head(19).index.tolist()\n",
    "display(len(drop_index))\n",
    "X_drop_train = X_train.drop(drop_index,axis=0)\n",
    "y_drop_train = y_train.drop(drop_index,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba26637",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_drop_train_transformed, X_drop_test_transformed, y_drop_train, y_drop_test = yX_transform(X_drop_train, X_test, y_drop_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1587f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_drop_train_transformed.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f41a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b41626",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.shape)\n",
    "display(X_train_transformed.shape)\n",
    "display(y_train.shape)\n",
    "\n",
    "\n",
    "display(X_drop_train.shape)\n",
    "display(X_drop_train_transformed.shape)\n",
    "display(y_drop_train.shape)\n",
    "\n",
    "\n",
    "display(X_test_transformed.shape)\n",
    "display(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR2 = LinearRegression().fit(X_drop_train_transformed, y_drop_train)\n",
    "\n",
    "y_pred2 = LR2.predict(X_drop_test_transformed)\n",
    "\n",
    "r2_score(y_drop_test, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0900e828",
   "metadata": {},
   "source": [
    "This appears to have significantly worsened the model!\n",
    "\n",
    "Perhaps as we are generating large errors on the high population validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49580bc7",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08457ed3",
   "metadata": {},
   "source": [
    "Using the original X dataset, I will resample the 'least_developed_countries_ldc' which make up 20-25% data. This shoudl improve the accuracy of the model in the range covered by these countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "train = pd.concat([X_train_transformed, y_train],axis=1)\n",
    "\n",
    "display(train.head())\n",
    "\n",
    "ldc = train[train['least_developed_countries_ldc']==1]\n",
    "non_ldc = train[train['least_developed_countries_ldc']==0]\n",
    "\n",
    "print('ldc:      ',len(ldc),' / ',round(100*len(ldc)/len(train),1),'%')\n",
    "print('non_ldc:  ',len(non_ldc),' /  ',round(100*len(non_ldc)/len(train),1),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10116eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldc_oversampled = resample(ldc, replace=True, n_samples = len(non_ldc), random_state=0)\n",
    "\n",
    "display(ldc_oversampled.shape)\n",
    "display(non_ldc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oversampled = pd.concat([ldc_oversampled,non_ldc],axis=0)\n",
    "y_train_over = train_oversampled['gdp_per_capita'].copy()\n",
    "X_train_over = train_oversampled.drop('gdp_per_capita',axis = 1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR3 = LinearRegression().fit(X_train_over, y_train_over)\n",
    "\n",
    "y_pred3 = LR3.predict(X_test_transformed)\n",
    "\n",
    "r2_score(y_test, y_pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d75e640",
   "metadata": {},
   "source": [
    "This yielded a small improvement to the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a96a5",
   "metadata": {},
   "source": [
    "### Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f382e5f1",
   "metadata": {},
   "source": [
    "I will now test different model families and then optimise the best one.\n",
    "\n",
    "We will use the oversampled training data as it gave a modest improvement on the LR3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fdd096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model1 = LinearRegression()\n",
    "model2 = DecisionTreeRegressor()\n",
    "model3 = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e61948",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = [model1, model2, model3]\n",
    "model_names = ['Linear Regression', 'Decision Tree Regressor', 'KNR']\n",
    "\n",
    "scores = {}\n",
    "for model, model_name in zip(model_pipeline, model_names): # loop through two parallel lists\n",
    "    mean_score = np.mean(cross_val_score(model, X_train_over, y_train_over, cv=5))\n",
    "    scores[model_name] = mean_score\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd909fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = [model1, model2, model3]\n",
    "model_names = ['Linear Regression', 'Decision Tree Regressor', 'KNR']\n",
    "\n",
    "scores = {}\n",
    "for model, model_name in zip(model_pipeline, model_names): # loop through two parallel lists\n",
    "    score = cross_val_score(model, X_train_over, y_train_over, cv=5)\n",
    "    scores[model_name] = score\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf684abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = {}\n",
    "for model, model_name in zip(model_pipeline, model_names):\n",
    "    model.fit(X_train_over, y_train_over)\n",
    "    val_scores[model_name] = model.score(X_test_transformed, y_test)\n",
    "print(val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bdec20",
   "metadata": {},
   "source": [
    "Something looks strange on the LR, perhaps overfitting?\n",
    "\n",
    "DecisionTreeRegressor is clearly the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b492c4a7",
   "metadata": {},
   "source": [
    "### Parameter optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad882ae",
   "metadata": {},
   "source": [
    "Using DTR, we can optimse further by tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f420b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "max_depth_choices= [3,4,5,6,7,8,9,10,None]\n",
    "criterion_choices = ['squared_error','absolute_error']\n",
    "min_samples_split_choices = [2,3,4,5,6,7,8,9,10]\n",
    "min_samples_leaf_choices = [1,2,3,4,5,6,7,8,9,10]\n",
    "max_features_choices = [2,3,4,5,6,None]\n",
    "\n",
    "grid = {'max_depth': max_depth_choices,\n",
    "               'criterion': criterion_choices,\n",
    "               'min_samples_split': min_samples_split_choices,\n",
    "               'min_samples_leaf': min_samples_leaf_choices,\n",
    "               'max_features': max_features_choices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "random_search = RandomizedSearchCV(estimator = model, param_distributions = grid, n_iter=500, cv = 5, n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed7d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "random_search.fit(X_train_over, y_train_over)\n",
    "\n",
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c1b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = DecisionTreeRegressor(min_samples_split= 2,\n",
    "                               min_samples_leaf= 1,\n",
    "                               max_features= None,\n",
    "                               max_depth= 9,\n",
    "                               criterion= 'squared_error').fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4 = model4.predict(X_test_transformed)\n",
    "\n",
    "r2_score(y_test, y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b41d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
